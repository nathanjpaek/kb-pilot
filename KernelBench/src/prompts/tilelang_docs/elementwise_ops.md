# ElementWise Operators in TileLang

Elementwise operators are widely used in deep learning and serve as excellent examples for understanding parallel programming with TileLang. This tutorial analyzes several implementations of the elementwise addition operator using TileLang. By the end of this tutorial, you will learn:

1. How to implement an elementwise operator using TileLang
2. How to compile operators with dynamic shapes
3. How TileLang addresses boundary-related issues
4. Advanced optimization techniques using TileLang constructs

## Basic Elementwise Addition in TileLang

```python
def elementwise_add(N, threads=256, dtype="bfloat16"):

    @T.prim_func
    def main(A: T.Tensor((N), dtype), B: T.Tensor((N), dtype), C: T.Tensor((N), dtype)):
        with T.Kernel(T.ceildiv(N, threads), threads=threads) as (b_x):
            # vector add.
            for i in T.Parallel(threads):
                C[b_x * threads + i] = A[b_x * threads + i] + B[b_x * threads + i]

    return main
```

All logic for TileLang kernels must be implemented within the `T.Kernel(...)` scope. In this example, initializing `T.kernel(...)` requires specifying both the grid size and the number of threads per block. The returned value `bx` corresponds to the block index. In the provided implementation, `T.Parallel` is used to process the data tile (of size `1 x threads`) assigned to the block for computation.

The code inside `T.Kernel` operates at the **block level**, not the thread level. Your focus is solely on defining the block-level logic. During compilation, TileLang automatically maps computations to the corresponding threads and applies further optimizations. The optimized code generated by TileLang aligns with carefully handcrafted computational logic. While TileLang also supports thread-level programming semantics, block-level programming provides a higher-level abstraction.

## Compilation and Execution

The program can be compiled using the following code:

```python
program = elementwise_add(1024, threads=256, dtype="bfloat16")
kernel = tilelang.compile(program, out_idx=-1, target="cuda", execution_backend="cython")
```

Launching the kernel is straightforward, just call it directly like a function:

```python
C = kernel(A, B)
```

## Two-Dimensional Elementwise Addition

The vector add operation can be extended to two-dimensional cases, where implementations demonstrate comparable efficiency. Below is an example:

```python
import tilelang.language as T
def elementwise_add(
    M,
    N,
    block_M,
    block_N,
    in_dtype,
    out_dtype,
    threads,
):
    @T.prim_func
    def main(
            A: T.Tensor((M, N), in_dtype),
            B: T.Tensor((M, N), in_dtype),
            C: T.Tensor((M, N), out_dtype),
    ):
        with T.Kernel(T.ceildiv(N, block_N), T.ceildiv(M, block_M), threads=threads) as (bx, by):
            start_x = bx * block_N
            start_y = by * block_M

            for (local_y, local_x) in T.Parallel(block_M, block_N):
                y = start_y + local_y
                x = start_x + local_x

                C[y, x] = A[y, x] + B[y, x]

    return main
```

## Dynamic Shape Compilation

In the compilation process above, a fixed shape was used. However, in practical usage, we often want the kernel to support dynamic shapes. In TileLang, we can replace the target size with a dynamic symbolic value, making the dimension dynamic:

```python
program = elementwise_add(T.symbolic("N"), threads=256, dtype="bfloat16")
kernel = tilelang.compile(program, out_idx=-1, target="cuda", execution_backend="cython")
```

The resulting compiled kernel will include an additional `int N` parameter alongside the tensor parameters.

## Boundary Condition Handling

TileLang automatically incorporates boundary-checking conditions; however, this comes at a cost. These boundary conditions may prevent TileLang from performing more advanced optimizations. 

When compiling the example below with `N` set to 2047:

```python
def elementwise_add(N, num_per_thread=8, threads=256, dtype="bfloat16"):

    @T.prim_func
    def main(A: T.Tensor((N), dtype), B: T.Tensor((N), dtype), C: T.Tensor((N), dtype)):
        with T.Kernel(T.ceildiv(N, threads * num_per_thread), threads=threads) as (b_x):
            # vector add.
            for i, j in T.Parallel(threads, num_per_thread):
                offsets = (b_x * threads + i) * num_per_thread
                C[offsets + j] = A[offsets + j] + B[offsets + j]

    return main
```

TileLang will generate boundary checking code to ensure memory safety. We can observe that TileLang may not apply optimizations such as vectorization or coalesced memory access when boundary conditions are complex. In fact, except for the tail group of data, all other threads could execute more optimized code.

## Advanced TileLang Optimizations

TileLang code written with loop-based traversal can be optimized by the compiler into highly efficient implementations, making it more accessible for beginners. The final generated code from the compiler remains observable, providing transparency into the optimization process.

While TileLang incorporates various optimizations, its behavior may sometimes appear counterintuitive. For example, when targeting 256 threads for task processing, applying vectorization can result in each thread computing 8 data elementsâ€”effectively utilizing only 32 active threads. The kernel launch configuration still retains the original allocation of 256 threads.

In such scenarios, explicitly specifying the number of elements computed per thread can help "guide" TileLang's code generation process:

```python
def elementwise_add(N, num_per_thread=8, threads=256, dtype="bfloat16"):

    @T.prim_func
    def main(A: T.Tensor((N), dtype), B: T.Tensor((N), dtype), C: T.Tensor((N), dtype)):
        with T.Kernel(T.ceildiv(N, threads * num_per_thread), threads=threads) as (b_x):
            # vector add.
            for i, j in T.Parallel(threads, num_per_thread):
                offsets = (b_x * threads + i) * num_per_thread
                C[offsets + j] = A[offsets + j] + B[offsets + j]

    return main
```

This approach results in generated code that aligns closely with conventional programming practices, making it more familiar and intuitive.

## Explicit Register Management

You can provide additional hints to TileLang by explicitly specifying register copies using the `T.copy(...)` operation. The example below demonstrates a vector addition implementation that explicitly loads data into registers before performing computations:

```python
def elementwise_add(N, NUM_ELE_PER_THREAD=8, threads=256, dtype="bfloat16"):

    @T.prim_func
    def main(A: T.Tensor((N), dtype), B: T.Tensor((N), dtype), C: T.Tensor((N), dtype)):
        with T.Kernel(T.ceildiv(N, threads * NUM_ELE_PER_THREAD), threads=threads) as (b_x):
            A_register = T.alloc_fragment((threads * NUM_ELE_PER_THREAD), dtype)
            B_register = T.alloc_fragment((threads * NUM_ELE_PER_THREAD), dtype)
            C_register = T.alloc_fragment((threads * NUM_ELE_PER_THREAD), dtype)

            s_start = b_x * threads * NUM_ELE_PER_THREAD
            s_end = (b_x + 1) * threads * NUM_ELE_PER_THREAD

            # Load data into registers
            T.copy(
                A[s_start:s_end],
                A_register,
            )
            T.copy(
                B[s_start:s_end],
                B_register,
            )

            # vector add.
            for tid, i in T.Parallel(threads, NUM_ELE_PER_THREAD):
                C_register[tid * NUM_ELE_PER_THREAD + i] = (
                    A_register[tid * NUM_ELE_PER_THREAD + i] +
                    B_register[tid * NUM_ELE_PER_THREAD + i])

            # Store results back to global memory
            T.copy(
                C_register,
                C[s_start:s_end],
            )

    return main
```

In this example, each thread is responsible for computing 8 elements. The `T.copy(...)` method functions at the block level, and TileLang automatically maps data movement operations to individual threads. This design provides fine-grained control over memory management and data movement patterns.

## Key TileLang Concepts

### T.Kernel Scope
All TileLang kernel logic must be within `T.Kernel(...)` scope, which defines the grid dimensions and thread count.

### T.Parallel
Used for parallel execution patterns. Can specify single or multiple dimensions for parallel loops.

### T.copy
Explicit data movement operations for loading/storing data between different memory spaces.

### T.alloc_fragment
Allocates register or shared memory fragments for intermediate computations.

### T.symbolic
Creates symbolic dimensions for dynamic shape support.

### T.ceildiv
Ceiling division utility for calculating grid dimensions.

These constructs provide the foundation for writing efficient parallel kernels in TileLang while maintaining high-level abstraction and compiler optimization opportunities.
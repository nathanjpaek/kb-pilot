TK_PAPER_PROMPT = """
ThunderKittens is a header-only C++-embedded GPU kernel framework built around tiles as the core data type, recognizing that modern GPUs like H100 and Blackwell are manycore processors optimized for small tile operations rather than massive matrices. You include a single header and write CUDA C++ that feels PyTorch-like while mapping directly to hardware features like WGMMA, TMA, and tensor cores. The framework uses explicit scope management, compile-time layout verification, and tile-first thinking to produce expressive, performant kernels.

The mental model centers on NVIDIA's hierarchical scopes. Threads handle individual elements but are rarely considered directly. Warps of 32 threads form the fundamental execution unit where operations naturally live, issuing instructions collaboratively and owning register tiles. Warpgroups of four warps coordinate asynchronous matrix multiplies via WGMMA for H100 and newer architectures. Blocks share memory across warps through shared tiles for producer-consumer patterns. Grids distribute work across SMs with persistent kernel strategies maximizing L2 cache reuse. Recent releases make scopes explicit via kittens::warp:: namespacing while maintaining tile-first abstraction.

The type system structures a memory hierarchy from Python tensors to GPU registers. Register tiles rt<T, rows, cols, layout> exist at warp scope for computation, parameterized by element type (bfloat16, fp16, fp8, float), shape, and layout (row or column major). These serve as your "vector registers" for compute. Shared tiles st<T, rows, cols> enable block-level data sharing with auto-selected swizzled layouts preventing bank conflicts while maintaining HGMMA/UTMA compatibility. TK chooses among three hardware-compatible layouts (32B, 64B, 128B stride families) at compile time based on tile width. Register vectors rv and shared vectors sv complement tiles for reductions and element-wise operations, coming in row or column flavors plus naive/aligned/orthogonal register layouts.

Global layouts gl<T, dims...> describe how you view 4-D HBM tensors for load/store operations, encoding element dtype and logical shape with batch, head, length, embed dimensions. Each dimension can be compile-time constant (saves registers via instruction cache) or runtime value (marked −1). You name per-kernel "globals" holding these layouts and index with 4-tuples during loads/stores to move tiles between HBM and shared or register memory. Parallel global layouts pgl<GL, NUM_DEVICES> extend this for multi-GPU operations via KittensClub thread pools.
ThunderKittens enforces strict compile-time layout compatibility through concepts and template metaprogramming. Tensor-core matrix multiplies require A in row-major and B in column-major; mismatches trigger compile-time errors rather than silent failures. This philosophy catches bugs during compilation, not runtime.

Every operation follows assembly-like destination-first syntax: operation(destination, source1, source2, ...). Arithmetic operations include add, mul, sub, div for element-wise math. Transcendental functions like exp2 compute efficiently. Reductions include row_sum and row_max. Data movement uses copy, load, store. Matrix math uses mma_AB and mma_ABt (trailing 't' indicates transpose), invoking tensor cores with required layout contracts. Vectors attached to tiles express row/column operations while TK selects underlying register layouts automatically.
Collaborative operations requiring multiple warps use templated group scopes like group<4>::mma_AB for warpgroup-level matrix operations, handling synchronization internally. The worker abstraction makes collaboration explicit—warps/warpgroups collectively own tiles and issue operations, with cooperative helpers for k warps performing asynchronous loads/stores.

The canonical execution pattern is LCSF: Load–Compute–Store–Finish, separating producer workers (I/O) from consumer workers (computation) for pipelined execution overlapping memory and math. Producer workers create and reuse TMA descriptors, issuing load_async/store_async to move tiles between HBM and shared memory via hardware acceleration. Consumer workers run WGMMA and other operations on register/shared tiles, synchronized through barriers and N-stage ring buffers allowing next iteration's loads during current iteration's compute. You implement four functions: load fetches data into shared memory, compute performs math in registers, store writes results to HBM, finish handles cleanup. Define per-block state including globals for tensor descriptors, input_block and finish_block for shared staging, common_state for shared data, and consumer_state for computation temporaries.

Pipeline stage count trades occupancy against register/SMEM pressure. TK defaults to four stages but you can tune based on arithmetic intensity and bandwidth. More stages increase throughput but consume more resources, potentially reducing concurrent blocks. Producer workers ideally operate at warpgroup scope for TMA throughput; consumer workers match tensor core operation granularity.
Grid/block strategies follow persistent-kernel mindset: launch long-running grids to amortize overhead and schedule blocks maximizing L2 reuse for your problem geometry. TK exposes helpers for tuning output tile sizes per block (e.g., 128×256 with two compute warpgroups vs 192×192 with three) and shaping grids to reuse data while avoiding tail-wave inefficiencies.

In practice, treat 16×16 tiles as fundamental vector registers. Keep tensor cores busy with mma calls on register tiles, maximizing instruction-level parallelism and register reuse. Overlap I/O and compute through multi-stage async pipelines. Index HBM with 4-tuples through global layouts, trusting static layout checks prevent illegal contracts. Shape grids to exploit L2 locality for your specific problem—attention, FFT convolutions, or rotary embeddings benefit from different tiling. Use compile-time dimensions wherever possible to save registers and improve instruction cache utilization.

For H100/Blackwell specifically: write at warpgroup scope for async matrix multiplies using group<4> operations, use TMA-backed async loads/stores for HBM↔SMEM movement, and rely on TK's managed swizzled layouts keeping WGMMA/UTMA pipelines conflict-free without manual padding. TK chooses optimal layouts at compile time based on tile dimensions.
"""
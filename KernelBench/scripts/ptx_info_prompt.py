# ~10,000 tokens

PTX_INFO_PROMPT = """
# PTX Kernel Writing Reference

PTX programs target NVIDIA GPUs as massively multithreaded coprocessors executing data-parallel kernels. Each kernel compiles to PTX intermediate representation and translates to target ISA at install or runtime. Understanding the execution model, memory architecture, type system, and instruction syntax is essential for writing correct PTX kernels by hand.

## Execution Model and Thread Organization

Threads execute in a three-level hierarchy. At the finest granularity, individual threads possess a three-component identifier tid with components tid.x, tid.y, and tid.z within their cooperative thread array. The CTA shape is defined by ntid with corresponding dimensional components. Threads group into CTAs (cooperative thread arrays), which are batched into grids. Within a CTA, threads coordinate through shared memory and barriers, executing in SIMT warps of machine-dependent size WARP_SZ, typically 32 threads. On SM 90 and later architectures, CTAs may optionally group into clusters that enable synchronization and shared memory communication across multiple CTAs within the cluster via cluster-wide barriers and shared memory windows.

Each CTA in a cluster exposes per-dimension CTA indices cluster_ctaid, cluster shape cluster_nctaid, linear rank cluster_ctarank, and total CTA count cluster_nctarank, accessible through special registers %cluster_ctaid, %cluster_nctaid, %cluster_ctarank, and %cluster_nctarank. Launch configurations may be explicit with user-specified cluster dimensions or implicit defaulting to 1×1×1, distinguished by the %is_explicit_cluster register. Code accessing peer CTA shared memory must verify the peer CTA's memory exists and the peer has not exited before performing cross-CTA shared memory operations.

Grids organize CTAs and clusters to achieve large thread counts. Threads in different clusters cannot communicate or synchronize. The system provides special registers for navigation: per-thread and per-CTA identifiers %tid and %ntid, grid-level CTA identifiers %ctaid and %nctaid, cluster identifiers %clusterid and %nclusterid, and temporal grid identifier %gridid. Grids may declare dependencies enabling ordered execution in constructs like CUDA Graphs.

## Memory Architecture and State Spaces

PTX treats memory as distinct named state spaces, each defining addressability, visibility, lifetime, and permitted instruction classes. Every variable declaration must specify a state space, which serves as both a storage location choice and an addressing contract determining which load, store, and atomic instructions are legal.

The register state space declared with .reg provides fast unaddressable storage private to each thread. Registers cannot have their addresses taken, and exceeding per-SM register budgets triggers automatic spilling to local memory. Special registers declared with .sreg are predefined read-only identifiers including %tid, %ctaid, %laneid, various timers, and performance counters. The constant state space .const provides read-only memory initialized by the host with a 64KB statically sized region plus ten additional 64KB driver-managed non-contiguous regions accessible via kernel parameters. Global memory .global offers context-wide read-write addressable storage defaulting to zero initialization, accessed through ld.global, st.global, and atom.global instructions. Local memory .local provides per-thread stack and private memory, which under the ABI resides on a stack frame and must be declared within function scope.

The parameter state space .param serves dual purposes: for kernel entry points, it holds read-only parameters shared across the grid and accessible via ld.param; for device functions, it passes by-value aggregates and return values where callers flatten data to local .param byte arrays using st.param before calling, and callees read via ld.param. Shared memory .shared is owned by the executing CTA, and on SM 90 and later can be addressed cluster-wide. Without explicit qualification, .shared implies .shared::cta scope. The mapa instruction computes .shared::cluster addresses for symbols residing in other CTAs within the cluster.

State space properties follow this pattern: .reg is not addressable, uninitialized, read-write, and per-thread; .sreg is not addressable, uninitialized, read-only, with platform-defined special register semantics; .const is addressable, host-initialized defaulting to zero, read-only, and per-grid; .global is addressable, initialized defaulting to zero, read-write, and context-wide; .local is addressable, uninitialized, read-write, and per-thread stack or private memory; .param for kernel entry is addressable only via ld.param, uninitialized, read-only, and per-grid; .param for device functions has restricted addressability, uninitialized, read-write, and per-thread scope; .shared::cta is addressable, uninitialized, read-write, and per-CTA; .shared::cluster is addressable, uninitialized, read-write, and visible within the cluster on SM 90 and later.

Opcode selection must match the state space: use ld.const for constant memory, ld.global and st.global for global memory, ld.shared and st.shared for shared memory, ld.local and st.local for local memory, and ld.param for kernel and function parameters. Cluster-wide shared memory requires ensuring peer CTA memory exists when accessed. For pointer parameters in .param space, annotations specify the target space and alignment using .ptr directives such as .param .u32 .ptr.global.align 16 gptr for a pointer targeting global memory with 16-byte alignment, or .param .u32 .ptr.const.align 8 cptr for const-space pointers, or .param .u32 .ptr.align 16 gen for generic pointers that may target const, global, local, or shared spaces.

## Source Format, Syntax, and Identifiers

PTX modules are ASCII text with newline-separated lines treating all whitespace as equivalent except for token separation. The C preprocessor can process PTX sources, recognizing lines beginning with # as preprocessor directives including #include, #define, #if, #ifdef, #else, #endif, #line, and #file. PTX is case-sensitive with lowercase keywords. Every module must begin with a .version directive specifying the PTX language version immediately followed by a .target directive specifying the target architecture.

Comments follow C and C++ syntax using /* and */ for non-nested multi-line comments and // for single-line comments extending to the next newline. Comments cannot appear within character constants, string literals, or other comments, and the parser treats them as whitespace. Statements are either directives or instructions, beginning with an optional label and ending with a semicolon. Directive keywords start with a dot to avoid conflicts with user identifiers. Available directives include .address_size, .alias, .align, .branchtargets, .callprototype, .calltargets, .common, .const, .entry, .explicitcluster, .extern, .file, .func, .global, .loc, .local, .maxclusterrank, .maxnctapersm, .maxnreg, .maxntid, .minnctapersm, .noreturn, .param, .pragma, .reg, .reqnctapercluster, .reqntid, .section, .shared, .sreg, .target, .tex, .version, .visible, and .weak.

Instruction format consists of an opcode followed by comma-separated operands and a semicolon. Operands may be register variables, constant expressions, address expressions, or label names. Instructions support optional guard predicates written as @p or @!p where p is a predicate register, enabling conditional execution. The guard predicate appears after any optional label but before the opcode. Destination operands precede source operands in the operand list.

User identifiers follow extended C++ rules in two forms: either starting with a letter a-z or A-Z followed by zero or more letters, digits, underscores, or dollar signs; or starting with an underscore, dollar, or percentage character followed by one or more letters, digits, underscores, or dollar signs. The regex patterns are followsym matching [a-zA-Z0-9_$] and identifier matching [a-zA-Z]{followsym}* or [$%]{followsym}+. PTX implementations should support identifiers of at least 1024 characters. The percentage sign as the first character distinguishes compiler-generated names from user-chosen names.

Predefined identifiers starting with % include %aggr_smem_size, %clock, %clock64, %cluster_ctaid, %cluster_ctarank, %cluster_nctaid, %cluster_nctarank, %clusterid, %ctaid, %current_graph_exec, %dynamic_smem_size, %envreg<32>, %globaltimer, %globaltimer_hi, %globaltimer_lo, %gridid, %is_explicit_cluster, %laneid, %lanemask_eq, %lanemask_ge, %lanemask_gt, %lanemask_le, %lanemask_lt, %nclusterid, %nctaid, %nsmid, %ntid, %nwarpid, %pm0 through %pm7, %reserved_smem_offset_begin, %reserved_smem_offset_cap, %reserved_smem_offset_end, %reserved_smem_offset<2>, %smid, %tid, %total_smem_size, %warpid, and the constant WARP_SZ.

All instruction keywords are reserved tokens including abs, activemask, add, addc, alloca, and, applypriority, atom, bar, barrier, bfe, bfi, bfind, bmsk, bra, brev, brkpt, brx, call, clz, cnot, copysign, cos, clusterlaunchcontrol, cp, createpolicy, cvt, cvta, discard, div, dp2a, dp4a, elect, ex2, exit, fence, fma, fns, getctarank, griddepcontrol, isspacep, istypep, ld, ldmatrix, ldu, lg2, lop3, mad, mad24, madc, mapa, match, max, mbarrier, membar, min, mma, mov, movmatrix, mul, mul24, multimem, nanosleep, neg, not, or, pmevent, popc, prefetch, prefetchu, prmt, rcp, red, redux, rem, ret, rsqrt, sad, selp, set, setmaxnreg, setp, shf, shfl, shl, shr, sin, slct, sqrt, st, stackrestore, stacksave, stmatrix, sub, subc, suld, suq, sured, sust, szext, tanh, tcgen05, tensormap, testp, tex, tld4, trap, txq, vabsdiff, vabsdiff2, vabsdiff4, vadd, vadd2, vadd4, vavrg2, vavrg4, vmad, vmax, vmax2, vmax4, vmin, vmin2, vmin4, vote, vset, vset2, vset4, vshl, vshr, vsub, vsub2, vsub4, wgmma, wmma, and xor.

## Constants and Expressions

All integer constants in PTX are 64-bit with types .s64 or .u64, with the signed versus unsigned distinction mattering for operations like division and ordered comparisons. When used in instructions or data initialization, constants convert to the appropriate size. Integer literals may be written in decimal as a nonzero digit followed by additional digits, hexadecimal as 0x or 0X followed by hex digits, octal as 0 followed by octal digits, or binary as 0b or 0B followed by binary digits. An optional U suffix marks unsigned interpretation. Literals without U suffix are signed .s64 unless the value exceeds the .s64 range, in which case they become unsigned .u64. The predefined constant WARP_SZ equals 32 on all current architectures.

Floating-point constants are represented as 64-bit double-precision values with all constant expressions evaluated using 64-bit double precision. The exception is 32-bit hex notation 0f or 0F followed by exactly 8 hex digits for exact single-precision values, which retain their exact 32-bit representation and cannot participate in constant expressions. Each 64-bit constant converts to the appropriate floating-point size at use. Literals may include an optional decimal point and optional signed exponent with no suffix letters, always representing 64-bit double precision. Hex notation allows exact IEEE 754 specification: 0f or 0F with 8 hex digits for single precision, 0d or 0D with 16 hex digits for double precision. For example, mov.f32 $f3, 0F3f800000 represents exactly 1.0 in single precision.

Predicate constants interpret integer constants as in C where zero is False and nonzero is True. Constant expressions use C-like operators with simplified fully-defined evaluation rules. Available operators include unary plus and minus, basic arithmetic addition, subtraction, multiplication, and division, comparison operators, conditional ternary operator, and parentheses. Integer expressions additionally support logical negation !, bitwise complement ~, remainder %, left and right shifts << and >>, bitwise operators &, |, and ^, and logical operators && and ||. Casts between integer and floating-point types are not supported. Operator precedence from highest to lowest: parentheses, unary operators including plus, minus, !, ~, and casts, multiplication, division, and remainder, addition and subtraction, shifts, ordered comparisons, equality comparisons, bitwise AND, bitwise XOR, bitwise OR, logical AND, logical OR, and conditional ternary. Unary operators associate right to left, binary operators left to right, and ternary operators right to left.

Integer expression evaluation proceeds as follows: literals are signed unless unsigned is required to prevent overflow or a U suffix is present, so 42, 0x1234, and 0123 are signed while 0xfabc123400000000, 42U, and 0x1234U are unsigned. Unary plus and minus preserve operand type. Logical negation ! produces signed results 0 or 1. Bitwise complement ~ interprets input as unsigned and produces unsigned output. Usual arithmetic conversions convert both operands to unsigned if either operand is unsigned. Addition, subtraction, multiplication, and division apply usual conversions with results matching the converted operand types. Remainder % treats operands as unsigned, differing from C semantics. Shifts interpret the second operand as unsigned with result type matching the first operand; right shift is arithmetic for signed values preserving the sign bit and logical for unsigned values shifting in zeros. Bitwise AND, OR, and XOR apply usual conversions with results matching converted types. Logical operators && and || along with equality operators == and != produce signed results 0 or 1. Ordered comparisons apply usual conversions and produce signed results 0 or 1. Casts to signed or unsigned use (.s64) and (.u64) syntax. For the conditional operator, the first operand must be integer, and the second and third operands must both be integers or both be floating point, with usual conversions applied to determine the result type.

## Type System and Declarations

Every instruction's behavior is determined by its type suffixes, using the same short specifiers in declarations. The fundamental type set includes signed integers .s8, .s16, .s32, .s64, unsigned integers .u8, .u16, .u32, .u64, floating point .f16, .f16x2, .f32, .f64, untyped bit types .b8, .b16, .b32, .b64, .b128, and predicates .pred. Type compatibility requires matching base kind and width. Bit types are width-compatible with anything of equal size and serve as payloads for packed and alternate formats. Sub-word arithmetic is restricted: .u8, .s8, and .b8 are legal for ld, st, cvt, and certain pack or unpack operations but not general ALU instructions. The .f16 type is allowed in conversions and half-precision instructions. The .f16x2 type is a true fundamental packed type, while other packed formats are instruction-specific types requiring bit-typed registers of appropriate total width.

Alternate floating formats for MMA, tensor pipelines, and quantization are not fundamental types. You declare registers as .b8, .b16, .b32, or .b64 and select the instruction type. For bf16, declare with .b16 register and use .bf16 or .bf16x2 instruction type. For e4m3 and e5m2, use .b8, .b16, or .b32 registers for packed x2 or x4 variants. For tf32, use .b32 register payload converting from .f32 with implementation-defined layout. For e2m1, pack as e2m1x2 in .b8. For e2m3 and e3m2, pack x2 in .b16 or x4 in .b32. For ue8m0, pack x2 in .b16 or use ue4m3 in .b8.

Packed data enables SIMD-within-register instructions. Only .f16x2 is fundamental; other formats require bit-typed operands sized to the whole pack. For .f16x2, use .f16x2 or .b32 registers; for .bf16x2, use .b32 registers; for .f32x2, use .b64 registers; for .e4m3x2, use .b16 registers and for .e4m3x4 use .b32 registers; for .e5m2x2, use .b16 registers and for .e5m2x4 use .b32 registers. Packed integers .u16x2 and .s16x2 use .b32 registers.

Variable declarations specify the state space, type possibly in vector or array form, the name, and optional .align directive as a power of two, initializer, or fixed address. Predicates may only reside in .reg space. Vectors declared with .v2 or .v4 apply to any non-predicate fundamental type limited to 128 bits total with default alignment equal to their total size for naturally aligned vector ld and st operations. Arrays use C-like name[dim][dim] syntax with dimension sizes as constant expressions, permitting omission of the first dimension if an initializer supplies the size.

Example declarations: .global .u32 loc declares a global unsigned 32-bit integer; .reg .s32 i declares a register-based signed 32-bit integer; .const .f32 bias[] = { -1.0, 1.0 } declares a read-only constant array; .global .u8 bg[4] = { 0,0,0,0 } declares a global byte array; .reg .v4 .f32 accel declares a 4-wide float vector in registers; .reg .pred p, q, r declares predicate registers; .local .u16 kernel[19][19] declares a per-thread stack array; .shared .u8 mailbox[128] declares CTA-shared memory; .global .v4 .b8 v declares 4 bytes packed in global memory. Parameterized register naming using .reg .b32 %r<100> generates registers %r0 through %r99.

Only .global and .const state spaces may be statically initialized; other spaces must be written at runtime. Uninitialized .global and .const variables default to zero. Initializers can use variable names as addresses, var+offset expressions, and the generic() operator to force a generic address. PTX 3.1 changed the default to space-relative offsets, requiring generic() when a generic pointer is needed. PTX 7.1 and later add a mask() operator extracting a single byte at a byte boundary from a symbol address or integer constant expression for building tables of low-byte or high-byte fields. Example initialization: .const .u32 foo = 42 sets a constant integer; .global .u32 bar[] = { 2,3,5 } initializes a global array; .global .u32 p1 = foo assigns the const-space offset of foo; .global .u32 p2 = generic(foo) assigns the generic address of foo; .global .u32 parr[] = { generic(bar), generic(bar)+4, generic(bar)+8 } creates a table of generic pointers; .global .u8 addr[] = { 0xFF(foo), 0xFF00(foo), 0xFF0000(foo) } uses mask to extract bytes; .global .u8 addr2[] = { 0xFF(generic(foo)+4), 0xFF00(1000+546) } combines mask with address arithmetic.

All memory access instructions require address alignment greater than or equal to the access size. Access size is the total bytes touched, for instance ld.v4.b32 has 16-byte access size and atom.f16x2 has 4-byte access size. Use .align to guarantee proper alignment, as in .const .align 4 .b8 bar[8] = {0,0,0,0,2,0,0,0} which declares a 4-byte aligned base with byte elements.

Attributes attach special properties to variables and functions. The .attribute(.managed) attribute marks unified virtual memory allocation for global variables on sm_30 and later. The .attribute(.unified(uuid_hi,uuid_lo)) attribute ensures the same address on host and device for global variables or device functions on sm_90 and later. Unified variables are read-only and must be loaded with .unified address operands; otherwise behavior is undefined.

## Operands, Addressing, and Type Conversion

All operands have known types from their declarations. Each operand type must be compatible with the instruction template and type without automatic conversion. Bit-size types are compatible with all types of the same size. Integer types of common size are compatible with each other. Operands with types different from but compatible with the instruction type are silently cast to the instruction type.

Source operands typically denoted a, b, and c must all reside in .reg state space for ALU instructions, as PTX implements a load-store machine. Most operations require consistent operand sizes. The cvt instruction accepts various operand types and sizes for conversion between nearly any data types. Instructions ld and st move data between addressable state spaces and registers, while mov copies data between registers. Most instructions support optional predicate guards for conditional execution, with predicate operands denoted p, q, r, and s.

Destination operands for instructions producing a single result store it in the d field, which must be a scalar or vector variable in .reg state space.

Address operands in memory instructions accept six forms: [var] for an addressable variable name, [reg] for a register containing a byte address with integer or bit-size type, [reg+immOff] for a register plus signed 32-bit constant byte offset, [var+immOff] for a variable address plus signed 32-bit constant byte offset, [immAddr] for an immediate absolute unsigned 32-bit byte address, and var[immOff] for array element access. Addresses must be naturally aligned to a multiple of the access size; improper alignment results in undefined behavior potentially causing silent masking of low-order address bits or instruction faults. Address sizes are 32-bit or 64-bit with 128-bit addresses unsupported. Addresses are zero-extended to the specified width as needed and truncated if register width exceeds the state space address width. Address arithmetic uses integer arithmetic and logical instructions with byte-based calculations, not C-style pointer arithmetic. The mov instruction moves variable addresses into pointers where the address is an offset in the variable's declared state space.

When a memory instruction omits state space specification, generic addressing applies. The state spaces .const, kernel function parameters .param, .local, and .shared are modeled as windows within the generic address space. Each window has a base and size equal to the corresponding state space size. Generic addresses map to global memory unless they fall within a const, local, or shared window. The kernel function parameters .param window is contained within the .global window. Within each window, a generic address maps to the underlying state space by subtracting the window base from the generic address.

Arrays of all types can be declared with the identifier becoming an address constant in the declaration space. Array size is a program constant. Array elements are accessed via explicitly calculated byte addresses or square-bracket indexing. The bracket expression can be a constant integer, register variable, or simple register with constant offset expression where the offset is a constant expression added to or subtracted from a register variable. More complex indexing requires prior address calculation. Examples include ld.global.u32 s, a[0] loading the first element, ld.global.u32 s, a[N-1] loading the last element, and mov.u32 s, a[1] moving the address of a[1] into s.

Vectors can be specified as source and destination operands. When used as destination operands, all elements in the vector expression must be unique or behavior is undefined. Vectors can be passed as function arguments. Vector elements are extracted using suffixes .x, .y, .z, .w or color fields .r, .g, .b, .a. Brace-enclosed lists pattern match to decompose vectors. Example mov.v4.f32 {a,b,c,d}, V extracts all four elements. Vector loads and stores enable wide memory operations for improved performance, accepting either a vector or brace-enclosed list of similarly typed scalars. Examples include ld.global.v4.f32 {a,b,c,d}, [addr+16] and ld.global.v2.u32 V2, [addr+8]. Element correspondence in brace-enclosed vectors {Ra, Rb, Rc, Rd} maps as Ra=V.x=V.r, Rb=V.y=V.g, Rc=V.z=V.b, Rd=V.w=V.a.

All operands to arithmetic, logic, and data movement instructions must have matching type and size except where changing size or type is part of the instruction definition. Different-sized or different-typed operands require prior conversion using cvt. The cvt instruction's behavior depends on source and destination types. For integer-to-integer conversions, s8/s16/s32/s64 to larger signed types use sign extension, to smaller types use chopping keeping only low bits that fit, and to unsigned types with different sizes also use chopping. Unsigned u8/u16/u32/u64 to larger types use zero extension and to smaller types use chopping. When the destination register is wider than the destination format after chopping, the result extends to the destination register width using extension type based on destination format, for instance cvt.s16.u32 targeting a 32-bit register first chops to 16 bits then sign-extends to 32 bits.

For signed-to-float conversions, all signed integer types convert to f16, f32, f64, or bf16. For unsigned-to-float conversions, all unsigned integer types convert to f16, f32, f64, or bf16. For float-to-signed conversions, f16, f32, f64, or bf16 convert to all signed integer types. For float-to-unsigned conversions, f16, f32, f64, or bf16 convert to all unsigned integer types. For float-to-float conversions, f16, f32, f64, and bf16 convert among each other and to or from e4m3, e5m2, e2m3, e3m2, and e2m1 types. Type bf16 converts to or from ue8m0. Type tf32 only converts to f32 and from f32. Conversions to floating point beyond the floating-point range are represented with maximum values, which are IEEE 754 Inf for f32 and f64 or approximately 131,000 for f16.

Conversion instructions may specify rounding. Six floating-point rounding modifiers exist: .rn rounds to nearest even, .rna rounds to nearest with ties away from zero, .rz rounds towards zero, .rm rounds towards negative infinity, .rp rounds towards positive infinity, and .rs rounds towards zero or away from zero based on carry out from integer addition of random bits and discarded mantissa bits. Four integer rounding modifiers exist: .rni rounds to nearest integer choosing even if equidistant, .rzi rounds to nearest integer towards zero, .rmi rounds to nearest integer towards negative infinity, and .rpi rounds to nearest integer towards positive infinity.

## Memory Consistency and Synchronization Model

PTX on sm_70 and later defines how memory effects from different threads can be observed in different orders. The model constrains which writes are eligible for reads to observe, ruling out specific contradictions between what threads see. Reads always return a value written to the same location including the implicit initial write, but the model defines which writes qualify. The model applies to all PTX ISA versions on sm_70 or newer but does not cover textures, surfaces, or ld.global.nc operations. When interacting with host memory, some system-scope atomics may not be fully atomic on all systems.

A memory operation is an ld, st, or atomic read-modify-write naming an address and data type. The memory location is the concrete byte range derived from the physical address and type size. Overlap means byte ranges intersect; complete overlap means identical ranges. Distinct virtual addresses mapping to one location are aliases. Multimem addresses target different physical locations across devices and must only be used with multimem instructions; other use is undefined. Vector memory operations are modeled as an unordered set of scalar operations on elements. Packed types act as two scalar operations to adjacent locations. Every byte starts with an implicit W0 write before any thread runs, representing the declared initializer or an unknown but constant byte.

The model's relations are state-space agnostic, but visibility depends on access reachability. For instance, ld.relaxed.shared.sys and ld.relaxed.shared.cluster have the same synchronizing effect because only cluster threads can reach the same shared locations. Operations are categorized by kind rather than raw mnemonics: atom and red are atomic operations; all ld plus the read part of atom are reads; all st and the write part of atom are writes; .volatile makes a volatile operation; .acquire, .release, or .acq_rel qualify acquire, release, or both; .mmio qualifies MMIO loads and stores; membar, fence.sc, and fence.acq_rel are memory-fence operations; fence.proxy or membar.proxy are proxy fences; any fence or any memory operation qualified .relaxed, .acquire, .release, .acq_rel, .volatile, or .mmio is a strong operation; ld.weak and st.weak are weak operations; barriers, fences, releases, and acquires are synchronizing operations.

MMIO operations (.mmio) behave as strong operations and if CUDA atomicity conditions are met at the given scope, writes are never combined and reads bypass caches and prefetching at that scope. Some implementations may fetch surrounding bytes in 32 to 128 byte ranges. Volatile operations (.volatile) are semantically relaxed system-scope with two constraints: the number of volatile instructions executed is preserved though hardware may merge or split underlying memory operations, and volatile instructions do not reorder around other volatile instructions even though actual memory operations can reorder. Use strong .relaxed.sys loads and stores for inter-thread synchronization rather than volatile, and use .mmio not volatile for device registers.

Every strong operation names a scope: .cta for threads in the same block, .cluster for threads in the same cluster, .gpu for all threads on the same device including other grids, .sys for all program threads across devices and the host. There is no warp scope. A proxy labels the access method, with generic memory using the generic proxy. Aliasing through distinct virtual addresses behaves as different proxies and requires a proxy fence to order them.

Two operations are morally strong if they are in program order within one thread, or both are strong and each operation's scope includes the other thread, and both use the same proxy, and if both are memory operations they overlap completely. Most axioms depend on moral strength. Conflicts are overlapping operations with at least one write. A data race is a conflicting pair that is not ordered by causality and not morally strong. If a race is mixed-size with partial overlap, the axioms do not apply. With only uniform-size races having complete overlap, the axioms suffice. Regardless, overlapping atomic RMWs with mutually inclusive scopes execute one entirely before the other even if they overlap only partially.

Release and acquire patterns are the synchronization building blocks. A release pattern on location M can be a release or acq_rel operation on M, or a release or acq_rel operation followed by a strong write to M in program order, or a release or acq_rel fence followed by a strong write to M. Its effect covers only operations occurring before the first instruction in that pattern for the issuing thread. An acquire pattern on M can be an acquire operation on M, or a strong read on M followed by an acquire operation on M in program order, or a strong read on M followed by an acquire fence. Its effect covers only operations after the last instruction in the pattern for the acquiring thread. Atomic reductions (red instructions) do not contribute the acquire read needed to form an acquire pattern.

Program order is the per-thread sequential order. Some instructions launch asynchronous operations like cp.async, cp.reduce.async.bulk, and wgmma.mma_async, where those async operations occur after prior instructions from the same thread except for the wgmma caveat but are not in program order. They come with narrower ordering guarantees described in their instruction documentation. Observation order chains a write to a read optionally via a sequence of atomic RMWs that the read observes. A write precedes a read if the read gets the write's value or the chain passes through atoms. Fence-SC order is a runtime partial order relating morally strong fence.sc operations and is acyclic.

Synchronization relations arise at runtime: fence.sc pairs synchronize if ordered by Fence-SC; CTA or cluster barrier arrives and waits synchronize as specified; a release pattern X and an acquire pattern Y synchronize if a write in X precedes a read in Y by observation order and the first operation of X and last operation of Y are morally strong. CUDA API actions also establish synchronization and participate in proxy-preserved base causality including task sequencing in streams, events and barriers, kernel and graph start and end relationships, API enqueue semantics, and synchronization APIs like cudaStreamSynchronize.

Causality order captures cross-thread visibility built from base causality, proxy-preserved base causality, and observation edges. Base causality is the transitive closure of program order and synchronizes-with along with their compositions. Proxy-preserved base causality adds the requirement that X and Y either touch the same address using the generic proxy, or the same address via the same non-generic proxy within the same CTA, or are aliases bridged by an alias proxy fence along the path. Full causality is proxy-preserved base causality plus: X precedes Y if X precedes some Z in observation order and Z precedes Y in proxy-preserved base causality.

Coherence order is a runtime partial order relating overlapping writes. Overlapping writes are coherent if they are morally strong or related by causality. Races break coherence between those writes yielding the partial nature. Communication order is a non-transitive runtime order expressing visibility: a write precedes an overlapping read if the read returns any byte from that write; a write precedes a later write if ordered earlier by coherence; a read precedes an overlapping write if the read returned a value from some write that is coherent-before the later write.

The axioms constrain executions. Coherence: if W precedes W′ in causality, then W precedes W′ in coherence. Fence-SC: Fence-SC may not contradict causality; if morally strong F1 precedes F2 in causality, then F1 precedes F2 in Fence-SC. Atomicity: single-copy atomicity forbids a morally strong read R from simultaneously observing W and any W′ coherent-before W on any overlapping byte; for atomic RMW A and write W that overlap and are morally strong, it cannot be both true that A reads from some W′ coherent-before W and that A follows W in coherence. Thin air: executions cannot conjure self-fulfilling values through speculative cycles. SC-per-location: among any overlapping pairwise morally strong operations, communication order cannot contradict program order ensuring per-location sequential consistency. Causality axiom: communication cannot contradict causality; if R precedes overlapping W in causality, R cannot read from W; if W precedes overlapping R in causality, R cannot read any write coherent-before W on the overlapping bytes.

## Instructions, Predicates, and Control Flow

Instructions consist of an opcode and between zero and four operands. The optional guard predicate prefixes the instruction with @p or @!p where p is a predicate register. Instructions lacking a guard predicate execute unconditionally by all threads. Predicate registers are declared using .reg .pred type as in .reg .pred p, q. The setp instruction sets predicates typically through comparisons. For example, setp.lt.s32 p, i, n followed by @p add.s32 j, j, 1 increments j only if i is less than n. The setp instruction can set two destination predicates simultaneously, one for the comparison result and one for its inverse, using syntax setp.lt.s32 p|q, a, b.

Comparison operators for setp include signed integer comparisons eq equal, ne not-equal, lt less-than, and gt greater-than, plus unsigned comparisons lo lower, ls lower-or-same, hi higher, and hs higher-or-same. Floating-point comparisons divide into ordered comparisons eq, ne, lt evaluating to false if any operand is NaN, and unordered comparisons equ, neu, ltu evaluating to true if any operand is NaN. Additionally, num checks if both operands are numeric not NaN, while nan tests if either operand is NaN.

Instructions in PTX are typed with the type specified as a suffix to the opcode such as add.u32 for 32-bit unsigned addition. Operands must conform to these types. Bit-size types .b16 and .b32 are compatible with any type of the same size. Signed and unsigned integers of the same width can be used together with silent casting. Floating-point types must match their instruction type exactly. Some instructions like cvt require multiple type modifiers specifying both source and destination types as in cvt.f32.u16 d, a.

When a source register is wider than the instruction type, its value is truncated to fit. When a destination register is wider than the instruction's output, the result is extended. For signed integer instruction types like .s16, values are sign-extended; for all other types including unsigned integers, bit-sizes, and floats, values are zero-extended.

Integer arithmetic includes add, sub, div, and rem. The mul and mad instructions offer modes to control the result. Using .lo or .hi returns the lower or upper half of the product fitting into a register of the same size as the operands. The .wide modifier for 16-bit and 32-bit types stores the full double-width product in a destination register twice the size of the source registers as in mul.wide.s32 d, a, b where d is 64-bit.

For floating-point arithmetic, the fma fused multiply-add instruction is fundamental for performance and precision, calculating a * b + c with a single rounding operation. On modern architectures, mad.f32 and mad.f64 are equivalent to fma. Floating-point instructions like add, mul, and fma support rounding modifiers .rn, .rz, .rm, .rp controlling numerical behavior. Omitting rounding modifiers can allow compiler optimization such as fusing mul and add into a single fma. The .ftz flush-to-zero modifier improves performance by treating subnormal numbers as zero. The .sat modifier saturates results to [0.0, 1.0] range. For activation functions, max and min instructions are essential; a ReLU can be implemented with max.f32 r0, r0, 0.0. Integer min and max instructions feature a .relu modifier clamping negative results to zero automatically.

Logical instructions and, or, xor, and not operate bitwise on integer or predicate registers. Shift instructions shl shift left and shr shift right take a 32-bit unsigned shift amount. For shr, when used with a signed type like .s32 it performs an arithmetic shift preserving the sign bit; with unsigned .u32 or bit-size .b32 types it performs a logical shift filling with zeros.

The mov instruction moves data between registers and can obtain the non-generic address of a variable in a specific memory space as in mov.u64 ptr, my_global_var. The mov instruction facilitates packing smaller vector elements into a larger scalar register such as mov.b32 r0, {h0, h1} packing two .u16 values, or unpacking in the reverse direction.

Memory is accessed via ld load and st store instructions requiring a state space qualifier .global, .shared, .local, or .const. For instance, ld.global.f32 d, [addr] loads a 32-bit float from global memory. These instructions can be augmented with cache operators .ca, .cg, .cs as performance hints to the memory system. The cvt instruction converts values between types such as cvt.f32.f16 f32_val, f16_val. Rounding modifiers are mandatory for any conversion that may lose precision including float-to-int, wider-float-to-narrower-float, and int-to-float operations.

Control flow uses bra branch, call, and ret instructions. Branching is conditional via a guard predicate. Both bra and call support a .uni suffix informing the compiler that the instruction is non-divergent where all threads in the warp take the same path, enabling significant optimizations. A ret instruction within the main kernel entry function terminates the thread as does the explicit exit instruction.

For coordinating threads, bar.sync creates a barrier causing threads within a CTA to wait until a specified number of threads reach it. Its syntax is bar.sync a, b where a is the barrier number and b is the participating thread count.

Atomic memory operations allow safe thread communication. The atom instruction atomically performs an operation like add, min, max, cas, or exch on a memory location in .global or .shared space, returning the original value at that location. Its syntax is atom.space.op.type d, [a], b. The red reduction instruction is similar but does not return the old value. Both atom and red support memory synchronization semantics .relaxed, .acquire, .release, .acq_rel and scopes .cta, .gpu, .sys defining ordering and visibility of memory operations across thread groups.

At the warp level, vote.sync allows threads to perform a reduction on a predicate. The .all mode returns true if the predicate is true for all participating threads, .any returns true if it is true for at least one, and .uni returns true if all threads have the same predicate value. The vote.sync.ballot.b32 d, p, mask form gathers the predicate p from each thread in the warp into a 32-bit bitmask in register d.

## Practical Kernel Writing Discipline

When writing whole PTX kernels, think in terms of explicit types and address spaces. Instruction suffixes like .s32, .u64, .f32, .f64 must match the register type you operate on. The assembler ptxas only complains at assemble time, so mismatches like using add.f64 on a 32-bit integer register will fail late. Declare registers with .reg using the correct width and kind, then maintain that contract across ld, arithmetic, and st operations. There are no native 8-bit registers; for byte loads and stores use ld.u8 or st.u8 with a wider integer register such as .u32 as the destination or source, then move or convert as needed.

Use the right memory space on every memory instruction. PTX does not infer spaces: choose ld.global and st.global for global memory, ld.shared and st.shared for shared memory. If porting code that relied on generic device pointers, you must still pick an address-space-specific ld or st in PTX. Insert memory fences explicitly using membar.gl, membar.cta when ordering is needed; the compiler will not add them automatically. If logic depends on seeing writes from other threads or CTAs, pair fences with appropriate synchronization primitives at the CUDA level or with PTX barriers.

Keep names unique and scopes clean. Inlined snippets can collide on register or label names. In whole PTX files, the same rule applies within a function: duplicate .reg names or labels in the same scope will conflict. A reliable pattern is to declare a block-local scratch register set with braces around a compound sequence and choose distinctive label prefixes for each control-flow region to avoid accidental reuse in large kernels.

Predication and conditional updates should be explicit. Declare predicate registers with .reg .pred %p, set them with setp, and predicate instructions using @%p or @!%p. If a value is conditionally updated, ensure the previous value is available in a register and only overwrite under the predicate so dataflow is clear.

Immediates and modes should be encoded directly in the instruction stream. For arithmetic with literals, prefer immediate forms when supported such as add.u32 %r1, %r1, 42. For floating-point rounding and flag modes, include the mode suffix on the instruction such as add.f32.rn versus add.f32.rz instead of relying on external state.

Integer registers map by width to .u16 corresponding to h-class, .u32 corresponding to r-class, .u64 corresponding to l-class, and .u128 corresponding to q-class if supported. Floating registers map .f32 to f-class and .f64 to d-class. In straight PTX you declare the right register class once with .reg then use instructions with matching suffixes.

Be meticulous with addresses and sizes. Pointer registers holding addresses are typically 64-bit, declared with .reg .u64 %rd<k> on modern GPUs, and used in address operands like [%rd1]. When moving between integer widths or between integer and float, use explicit cvt instructions with correct signedness and size in both source and destination type fields as in cvt.f32.s64 %f1, %rd1. Do not rely on implicit extension or truncation.

Terminate each instruction with a semicolon. Use line comments // or block comments /* … */. Group temporary declarations and the computation using them closely. A readable micro-template for a small compute fragment is opening brace, .reg declarations, computation, closing brace, which mirrors the declare temporary, compute, consume, leave scope pattern and prevents symbol clashes as the kernel grows.

Volatile and reordering considerations move into PTX qualifiers not host clobbers. If a memory operation must not be optimized away or reordered past certain points, use ld.volatile or st.volatile or explicit fences. The C and C++ volatile asm and memory clobber concepts do not exist in standalone PTX files, so you must encode intent with PTX qualifiers and barriers.

Error surfaces are late so validate types early. Because ptxas is the first stage that truly parses PTX, many mistakes only show up at assemble time. The safest practice is to write with tight mechanical correspondence: for every register declaration, ensure all consuming instructions carry the matching type suffix; for every load or store, ensure the address space and data width are correct; for every conversion, spell both sides' types in the mnemonic as in cvt with destination and source types.

A minimal concrete example loading two 32-bit integers from global memory, adding them, and storing the result follows this discipline. Start with .version 8.0, .target sm_80, and .address_size 64. Declare the entry function as .visible .entry add_kernel with .param .u64 inA, .param .u64 inB, .param .u64 outC parameters. Inside the function body, declare .reg .u64 %rdA, %rdB, %rdC for 64-bit address registers and .reg .u32 %rA, %rB, %rC for 32-bit data registers. Load parameters with ld.param.u64 %rdA, [inA], ld.param.u64 %rdB, [inB], ld.param.u64 %rdC, [outC]. Load global data with ld.global.u32 %rA, [%rdA] and ld.global.u32 %rB, [%rdB]. Perform the addition with add.s32 %rC, %rA, %rB. Store the result with st.global.u32 [%rdC], %rC. Terminate with ret.

When translating inline PTX idioms to whole-kernel PTX, map constraint letters to choosing the right .reg class up front; the %% percent escape for inline assembly is irrelevant in .ptx files where registers are named with % normally; memory clobbers and volatile asm directives translate to using volatile on PTX loads and stores and inserting membar instructions; braces to avoid duplicate temporaries and labels translate to keeping temps and labels confined and uniquely named within function scope. Following these concrete rules keeps PTX valid, predictable, and extensible.
"""